---
phase: 08-backend-provider-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/caal/llm/providers/openai_compatible_provider.py
autonomous: true

must_haves:
  truths:
    - "Developer can instantiate OpenAI-compatible provider with custom base URL"
    - "Developer can optionally provide API key for authenticated servers"
    - "Provider streams text responses correctly"
    - "Provider executes tool calls and returns normalized ToolCall objects"
  artifacts:
    - path: "src/caal/llm/providers/openai_compatible_provider.py"
      provides: "OpenAICompatibleProvider class"
      exports: ["OpenAICompatibleProvider"]
      min_lines: 100
  key_links:
    - from: "src/caal/llm/providers/openai_compatible_provider.py"
      to: "src/caal/llm/providers/base.py"
      via: "extends LLMProvider"
      pattern: "class OpenAICompatibleProvider\\(LLMProvider\\)"
    - from: "src/caal/llm/providers/openai_compatible_provider.py"
      to: "openai.AsyncOpenAI"
      via: "uses AsyncOpenAI client"
      pattern: "AsyncOpenAI\\("
---

<objective>
Implement OpenAI-compatible LLM provider that works with any server exposing OpenAI-compatible API (LiteLLM, vLLM, LocalAI, text-generation-inference).

Purpose: Enable CAAL to use a wide variety of self-hosted and third-party LLM servers that follow the OpenAI API specification.
Output: `OpenAICompatibleProvider` class implementing `LLMProvider` interface with streaming and tool calling support.
</objective>

<execution_context>
@/Users/mmaudet/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mmaudet/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-backend-provider-foundation/08-RESEARCH.md

# Reference implementation to follow exactly
@src/caal/llm/providers/groq_provider.py
@src/caal/llm/providers/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create OpenAICompatibleProvider class</name>
  <files>src/caal/llm/providers/openai_compatible_provider.py</files>
  <action>
Create `OpenAICompatibleProvider` class following the GroqProvider pattern exactly:

1. **Imports**: Use `from openai import AsyncOpenAI` (already installed, v2.8.1+)

2. **Class definition**:
   - Extend `LLMProvider` from `.base`
   - Docstring explaining supported servers (LiteLLM, vLLM, LocalAI, etc.)

3. **__init__ parameters**:
   - `model: str` - Model name (server-specific, e.g., "gpt-3.5-turbo" or "mistral")
   - `base_url: str` - Server URL (e.g., "http://localhost:8000/v1")
   - `api_key: str | None = None` - Optional API key (use "not-needed" as placeholder if None, some servers reject empty string)
   - `temperature: float = 0.7`
   - `max_tokens: int = 4096`

4. **__init__ implementation**:
   - Store all parameters as private attributes
   - Strip trailing slash from base_url: `base_url = base_url.rstrip("/")`
   - Create `self._client = AsyncOpenAI(base_url=base_url, api_key=api_key or "not-needed")`
   - Log initialization with debug level

5. **Properties**:
   - `provider_name` -> return `"openai_compatible"`
   - `model` -> return `self._model`
   - `temperature` -> return `self._temperature`

6. **chat() method** - Non-streaming with tool support:
   - Build request_kwargs dict with model, messages, temperature, max_tokens, stream=False
   - If tools provided: add `tools=tools`, `tool_choice="auto"`
   - Call `await self._client.chat.completions.create(**request_kwargs)`
   - Extract message from `response.choices[0].message`
   - Extract tool calls using `parse_tool_arguments()` for each `tc.function.arguments`
   - Return `LLMResponse(content=message.content, tool_calls=tool_calls)`

7. **chat_stream() method** - Streaming for text responses:
   - Build request_kwargs with stream=True
   - CRITICAL: If tools provided, add `tools=tools` and `tool_choice="none"` (prevents silent sessions)
   - Create stream with `await self._client.chat.completions.create(**request_kwargs)`
   - Yield content: `async for chunk in stream: if chunk.choices and chunk.choices[0].delta.content: yield chunk.choices[0].delta.content`

8. **parse_tool_arguments() method**:
   - Handle both string (JSON) and dict inputs
   - Try `json.loads()` for strings, catch JSONDecodeError and return {}
   - Return dict directly if already dict
   - Log warning on parse failure

9. **format_tool_result() method**:
   - Include `name` field (required by OpenAI API, like Groq)
   - Return: `{"role": "tool", "content": content, "tool_call_id": tool_call_id, "name": tool_name}`

10. **Module exports**: Add `__all__ = ["OpenAICompatibleProvider"]`
  </action>
  <verify>
Run type checker and linter:
```bash
uv run mypy src/caal/llm/providers/openai_compatible_provider.py
uv run ruff check src/caal/llm/providers/openai_compatible_provider.py
```
Both commands should pass with no errors.
  </verify>
  <done>
- OpenAICompatibleProvider class exists and implements LLMProvider interface
- Type checking passes (mypy)
- Linting passes (ruff)
- Class has all required methods: chat(), chat_stream(), parse_tool_arguments(), format_tool_result()
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify provider instantiation and basic functionality</name>
  <files>src/caal/llm/providers/openai_compatible_provider.py</files>
  <action>
Create a quick verification that the provider can be imported and instantiated:

1. Run Python to verify import works:
```bash
uv run python -c "from caal.llm.providers.openai_compatible_provider import OpenAICompatibleProvider; p = OpenAICompatibleProvider(model='test', base_url='http://localhost:8000/v1'); print(f'Provider: {p.provider_name}, Model: {p.model}')"
```

2. Verify the class structure matches expected interface by checking properties exist.

If import fails or instantiation fails, fix the issues in the provider file.
  </action>
  <verify>
The Python command should output:
```
Provider: openai_compatible, Model: test
```
No import errors or instantiation errors.
  </verify>
  <done>
- Provider can be imported from its module
- Provider can be instantiated with model and base_url
- provider_name property returns "openai_compatible"
- model property returns the provided model name
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. File exists: `src/caal/llm/providers/openai_compatible_provider.py`
2. Type checking passes: `uv run mypy src/caal/llm/providers/openai_compatible_provider.py`
3. Linting passes: `uv run ruff check src/caal/llm/providers/openai_compatible_provider.py`
4. Import works: `from caal.llm.providers.openai_compatible_provider import OpenAICompatibleProvider`
5. Instantiation works with base_url parameter
</verification>

<success_criteria>
- OpenAICompatibleProvider class implements LLMProvider interface
- Accepts custom base_url and optional api_key
- Has streaming support with tool_choice="none" pattern
- Has non-streaming tool calling support
- Type safe (mypy passes)
- Style compliant (ruff passes)
</success_criteria>

<output>
After completion, create `.planning/phases/08-backend-provider-foundation/08-01-SUMMARY.md`
</output>
