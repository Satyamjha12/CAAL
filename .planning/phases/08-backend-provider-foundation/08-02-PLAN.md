---
phase: 08-backend-provider-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/caal/llm/providers/openrouter_provider.py
autonomous: true

must_haves:
  truths:
    - "Developer can instantiate OpenRouter provider with API key"
    - "Provider uses fixed OpenRouter base URL (https://openrouter.ai/api/v1)"
    - "Provider streams text responses correctly"
    - "Provider executes tool calls and returns normalized ToolCall objects"
  artifacts:
    - path: "src/caal/llm/providers/openrouter_provider.py"
      provides: "OpenRouterProvider class"
      exports: ["OpenRouterProvider"]
      min_lines: 100
  key_links:
    - from: "src/caal/llm/providers/openrouter_provider.py"
      to: "src/caal/llm/providers/base.py"
      via: "extends LLMProvider"
      pattern: "class OpenRouterProvider\\(LLMProvider\\)"
    - from: "src/caal/llm/providers/openrouter_provider.py"
      to: "openai.AsyncOpenAI"
      via: "uses AsyncOpenAI client with OpenRouter base URL"
      pattern: "base_url.*openrouter\\.ai"
---

<objective>
Implement OpenRouter LLM provider for accessing 400+ cloud models through OpenRouter's unified API.

Purpose: Enable CAAL to use any model available on OpenRouter (GPT-4, Claude, Llama, Mistral, etc.) with a single API key.
Output: `OpenRouterProvider` class implementing `LLMProvider` interface with streaming and tool calling support.
</objective>

<execution_context>
@/Users/mmaudet/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mmaudet/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-backend-provider-foundation/08-RESEARCH.md

# Reference implementation to follow exactly
@src/caal/llm/providers/groq_provider.py
@src/caal/llm/providers/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create OpenRouterProvider class</name>
  <files>src/caal/llm/providers/openrouter_provider.py</files>
  <action>
Create `OpenRouterProvider` class following the GroqProvider pattern with OpenRouter-specific configuration:

1. **Imports**: Use `from openai import AsyncOpenAI` (already installed)

2. **Constants** at module level:
   ```python
   OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
   ```

3. **Class definition**:
   - Extend `LLMProvider` from `.base`
   - Docstring explaining OpenRouter's 400+ models, link to openrouter.ai

4. **__init__ parameters**:
   - `model: str` - OpenRouter model ID (e.g., "openai/gpt-4", "anthropic/claude-3-opus")
   - `api_key: str` - REQUIRED (OpenRouter always requires authentication)
   - `temperature: float = 0.7`
   - `max_tokens: int = 4096`

5. **__init__ implementation**:
   - Validate API key is provided (raise ValueError if not)
   - Store all parameters as private attributes
   - Create client with OpenRouter-specific headers:
     ```python
     self._client = AsyncOpenAI(
         base_url=OPENROUTER_BASE_URL,
         api_key=api_key,
         default_headers={
             "HTTP-Referer": "https://github.com/your-org/caal",
             "X-Title": "CAAL Voice Assistant",
         },
     )
     ```
   - Log initialization with debug level

6. **Properties**:
   - `provider_name` -> return `"openrouter"`
   - `model` -> return `self._model`
   - `temperature` -> return `self._temperature`

7. **chat() method** - Non-streaming with tool support:
   - Build request_kwargs dict with model, messages, temperature, max_tokens, stream=False
   - If tools provided: add `tools=tools`, `tool_choice="auto"`
   - Call `await self._client.chat.completions.create(**request_kwargs)`
   - Extract message from `response.choices[0].message`
   - Extract tool calls using `parse_tool_arguments()` for each `tc.function.arguments`
   - Return `LLMResponse(content=message.content, tool_calls=tool_calls)`

8. **chat_stream() method** - Streaming for text responses:
   - Build request_kwargs with stream=True
   - CRITICAL: If tools provided, add `tools=tools` and `tool_choice="none"` (prevents silent sessions)
   - Create stream with `await self._client.chat.completions.create(**request_kwargs)`
   - Yield content: `async for chunk in stream: if chunk.choices and chunk.choices[0].delta.content: yield chunk.choices[0].delta.content`

9. **parse_tool_arguments() method**:
   - Handle both string (JSON) and dict inputs
   - Try `json.loads()` for strings, catch JSONDecodeError and return {}
   - Return dict directly if already dict
   - Log warning on parse failure

10. **format_tool_result() method**:
    - Include `name` field (required by OpenAI-compatible API)
    - Return: `{"role": "tool", "content": content, "tool_call_id": tool_call_id, "name": tool_name}`

11. **Module exports**: Add `__all__ = ["OpenRouterProvider"]`
  </action>
  <verify>
Run type checker and linter:
```bash
uv run mypy src/caal/llm/providers/openrouter_provider.py
uv run ruff check src/caal/llm/providers/openrouter_provider.py
```
Both commands should pass with no errors.
  </verify>
  <done>
- OpenRouterProvider class exists and implements LLMProvider interface
- Uses fixed OPENROUTER_BASE_URL constant
- Requires API key (raises ValueError if not provided)
- Includes attribution headers (HTTP-Referer, X-Title)
- Type checking passes (mypy)
- Linting passes (ruff)
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify provider instantiation and API key validation</name>
  <files>src/caal/llm/providers/openrouter_provider.py</files>
  <action>
Verify the provider can be imported, instantiated correctly, and validates API key:

1. Run Python to verify import and instantiation works:
```bash
uv run python -c "from caal.llm.providers.openrouter_provider import OpenRouterProvider; p = OpenRouterProvider(model='openai/gpt-4', api_key='test-key'); print(f'Provider: {p.provider_name}, Model: {p.model}')"
```

2. Verify API key validation - should raise ValueError when no key provided:
```bash
uv run python -c "
from caal.llm.providers.openrouter_provider import OpenRouterProvider
try:
    p = OpenRouterProvider(model='openai/gpt-4', api_key='')
    print('ERROR: Should have raised ValueError')
except ValueError as e:
    print(f'OK: ValueError raised - {e}')
"
```

If import fails or validation behavior is incorrect, fix the issues in the provider file.
  </action>
  <verify>
The first command should output:
```
Provider: openrouter, Model: openai/gpt-4
```

The second command should output:
```
OK: ValueError raised - ...
```
  </verify>
  <done>
- Provider can be imported from its module
- Provider can be instantiated with model and api_key
- Provider raises ValueError when api_key is empty or None
- provider_name property returns "openrouter"
- model property returns the provided model name
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. File exists: `src/caal/llm/providers/openrouter_provider.py`
2. Type checking passes: `uv run mypy src/caal/llm/providers/openrouter_provider.py`
3. Linting passes: `uv run ruff check src/caal/llm/providers/openrouter_provider.py`
4. Import works: `from caal.llm.providers.openrouter_provider import OpenRouterProvider`
5. Instantiation works with api_key parameter
6. ValueError raised when api_key is missing
</verification>

<success_criteria>
- OpenRouterProvider class implements LLMProvider interface
- Uses fixed OpenRouter base URL
- Requires API key (validation on init)
- Includes attribution headers
- Has streaming support with tool_choice="none" pattern
- Has non-streaming tool calling support
- Type safe (mypy passes)
- Style compliant (ruff passes)
</success_criteria>

<output>
After completion, create `.planning/phases/08-backend-provider-foundation/08-02-SUMMARY.md`
</output>
